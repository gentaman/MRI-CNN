{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/genta/.pyenv/versions/anaconda-4.0.0/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load data =  11.613603830337524\n",
      "X_train.shape at input =  (40000, 64, 64, 2)\n",
      "Y_train.shape at input =  (40000, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# %load myAutomap.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "import time\n",
    "from generate_input import load_images_from_folder\n",
    "\n",
    "\n",
    "# Load training data:\n",
    "tic1 = time.time()\n",
    "dir_train = '/home/genta/dataset/tiny-imagenet-200/test/images'  # Folder with images\n",
    "n_im = 10000  # How many images to load\n",
    "X_train, Y_train = load_images_from_folder(  # Load images for training\n",
    "    dir_train,\n",
    "    n_im,\n",
    "    normalize=False,\n",
    "    imrotate=True)\n",
    "toc1 = time.time()\n",
    "print('Time to load data = ', (toc1 - tic1))\n",
    "print('X_train.shape at input = ', X_train.shape)\n",
    "print('Y_train.shape at input = ', Y_train.shape)\n",
    "\n",
    "\n",
    "def create_placeholders(n_H0, n_W0):\n",
    "    \"\"\" Creates placeholders for x and y for tf.session\n",
    "    :param n_H0: image height\n",
    "    :param n_W0: image width\n",
    "    :return: x and y - tf placeholders\n",
    "    \"\"\"\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_H0, n_W0, 2], name='x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, n_H0, n_W0], name='y')\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\" Initializes filters for the convolutional and de-convolutional layers\n",
    "    :return: parameters - a dictionary of filters (W1 - first convolutional\n",
    "    layer, W2 - second convolutional layer, W3 - de-convolutional layer\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = tf.get_variable(\"W1\", [5, 5, 1, 64],  # 64 filters of size 5x5\n",
    "                         initializer=tf.contrib.layers.xavier_initializer\n",
    "                         (seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", [5, 5, 64, 64],  # 64 filters of size 5x5\n",
    "                         initializer=tf.contrib.layers.xavier_initializer\n",
    "                         (seed=0))\n",
    "    W3 = tf.get_variable(\"W3\", [7, 7, 1, 64],  # 64 filters of size 7x7\n",
    "                         initializer=tf.contrib.layers.xavier_initializer\n",
    "                         (seed=0))  # conv2d_transpose\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2,\n",
    "                  \"W3\": W3}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def forward_propagation(x, parameters):\n",
    "    \"\"\" Defines all layers for forward propagation:\n",
    "    Fully connected (FC1) -> tanh activation: size (n_im, n_H0 * n_W0)\n",
    "    -> Fully connected (FC2) -> tanh activation:  size (n_im, n_H0 * n_W0)\n",
    "    -> Convolutional -> ReLU activation: size (n_im, n_H0, n_W0, 64)\n",
    "    -> Convolutional -> ReLU activation with l1 regularization: size (n_im, n_H0, n_W0, 64)\n",
    "    -> De-convolutional: size (n_im, n_H0, n_W0)\n",
    "    :param x: Input - images in frequency space, size (n_im, n_H0, n_W0, 2)\n",
    "    :param parameters: parameters of the layers (e.g. filters)\n",
    "    :return: output of the last layer of the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    x_temp = tf.contrib.layers.flatten(x)  # size (n_im, n_H0 * n_W0 * 2)\n",
    "    n_out = np.int(x.shape[1] * x.shape[2])  # size (n_im, n_H0 * n_W0)\n",
    "    print(tf.AUTO_REUSE)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        # FC: input size (n_im, n_H0 * n_W0 * 2), output size (n_im, n_H0 * n_W0)\n",
    "        FC1 = tf.contrib.layers.fully_connected(\n",
    "            x_temp,\n",
    "            n_out,\n",
    "            activation_fn=tf.tanh,\n",
    "            normalizer_fn=None,\n",
    "            normalizer_params=None,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            weights_regularizer=None,\n",
    "            biases_initializer=None,\n",
    "            biases_regularizer=None,\n",
    "            reuse=tf.AUTO_REUSE,\n",
    "#            reuse=True,\n",
    "            variables_collections=None,\n",
    "            outputs_collections=None,\n",
    "            trainable=True,\n",
    "            scope='fc1')\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        # FC: input size (n_im, n_H0 * n_W0), output size (n_im, n_H0 * n_W0)\n",
    "        FC2 = tf.contrib.layers.fully_connected(\n",
    "            FC1,\n",
    "            n_out,\n",
    "            activation_fn=tf.tanh,\n",
    "            normalizer_fn=None,\n",
    "            normalizer_params=None,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            weights_regularizer=None,\n",
    "            biases_initializer=None,\n",
    "            biases_regularizer=None,\n",
    "            reuse=tf.AUTO_REUSE,\n",
    "#            reuse=True,\n",
    "            variables_collections=None,\n",
    "            outputs_collections=None,\n",
    "            trainable=True,\n",
    "            scope='fc2')\n",
    "\n",
    "    # Reshape output from FC layers into array of size (n_im, n_H0, n_W0, 1):\n",
    "    FC_M = tf.reshape(FC2, [tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], 1])\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\":\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "\n",
    "    # CONV2D: filters W1, stride of 1, padding 'SAME'\n",
    "    # Input size (n_im, n_H0, n_W0, 1), output size (n_im, n_H0, n_W0, 64)\n",
    "    Z1 = tf.nn.conv2d(FC_M, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # RELU\n",
    "    CONV1 = tf.nn.relu(Z1)\n",
    "\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    # Input size (n_im, n_H0, n_W0, 64), output size (n_im, n_H0, n_W0, 64)\n",
    "    # Z2 = tf.nn.conv2d(CONV1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # RELU\n",
    "    # CONV2 = tf.nn.relu(Z2)\n",
    "    CONV2 = tf.layers.conv2d(\n",
    "        CONV1,\n",
    "        filters=64,\n",
    "        kernel_size=5,\n",
    "        strides=(1, 1),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "        dilation_rate=(1, 1),\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=None,\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        kernel_regularizer=tf.contrib.layers.l1_regularizer(0.0001),\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        trainable=True,\n",
    "        name='conv2',\n",
    "        reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    # DE-CONV2D: filters W3, stride 1, padding 'SAME'\n",
    "    # Input size (n_im, n_H0, n_W0, 64), output size (n_im, n_H0, n_W0, 1)\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    deconv_shape = tf.stack([batch_size, x.shape[1], x.shape[2], 1])\n",
    "    DECONV = tf.nn.conv2d_transpose(CONV2, W3, output_shape=deconv_shape,\n",
    "                                    strides=[1, 1, 1, 1], padding='SAME')\n",
    "    DECONV = tf.squeeze(DECONV)\n",
    "\n",
    "    return DECONV\n",
    "\n",
    "\n",
    "def compute_cost(DECONV, Y):\n",
    "    \"\"\"\n",
    "    Computes cost (squared loss) between the output of forward propagation and\n",
    "    the label image\n",
    "    :param DECONV: output of forward propagation\n",
    "    :param Y: label image\n",
    "    :return: cost (squared loss)\n",
    "    \"\"\"\n",
    "\n",
    "    cost = tf.square(DECONV - Y)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def random_mini_batches(x, y, mini_batch_size=64, seed=0):\n",
    "    \"\"\" Shuffles training examples and partitions them into mini-batches\n",
    "    to speed up the gradient descent\n",
    "    :param x: input frequency space data\n",
    "    :param y: input image space data\n",
    "    :param mini_batch_size: mini-batch size\n",
    "    :param seed: can be chosen to keep the random choice consistent\n",
    "    :return: a mini-batch of size mini_batch_size of training examples\n",
    "    \"\"\"\n",
    "\n",
    "    m = x.shape[0]  # number of input images\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Shuffle (x, y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = x[permutation, :]\n",
    "    shuffled_Y = y[permutation, :]\n",
    "\n",
    "    # Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(math.floor(\n",
    "        m / mini_batch_size))  # number of mini batches of size mini_batch_size\n",
    "\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size:k * mini_batch_size\n",
    "                                    + mini_batch_size, :, :, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size:k * mini_batch_size\n",
    "                                    + mini_batch_size, :, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches\n",
    "                                  * mini_batch_size: m, :, :, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches\n",
    "                                  * mini_batch_size: m, :, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def model(X_train, Y_train, learning_rate=0.0001,\n",
    "          num_epochs=100, minibatch_size=64, print_cost=True):\n",
    "    \"\"\" Runs the forward and backward propagation\n",
    "    :param X_train: input training frequency-space data\n",
    "    :param Y_train: input training image-space data\n",
    "    :param learning_rate: learning rate of gradient descent\n",
    "    :param num_epochs: number of epochs\n",
    "    :param minibatch_size: size of mini-batch\n",
    "    :param print_cost: if True - the cost will be printed every epoch, as well\n",
    "    as how long it took to run the epoch\n",
    "    :return: this function saves the model to a file. The model can then\n",
    "    be used to reconstruct the image from frequency space\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        ops.reset_default_graph()  # to not overwrite tf variables\n",
    "        seed = 3\n",
    "        (m, n_H0, n_W0, _) = X_train.shape\n",
    "\n",
    "        # Create Placeholders\n",
    "        X, Y = create_placeholders(n_H0, n_W0)\n",
    "\n",
    "        # Initialize parameters\n",
    "        parameters = initialize_parameters()\n",
    "\n",
    "        # Build the forward propagation in the tf graph\n",
    "        DECONV = forward_propagation(X, parameters)\n",
    "\n",
    "        # Add cost function to tf graph\n",
    "        cost = compute_cost(DECONV, Y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "        # Initialize all the variables globally\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Add ops to save and restore all the variables\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # For memory\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "\n",
    "        # Memory config\n",
    "        #config = tf.ConfigProto()\n",
    "        #config.gpu_options.allow_growth = True\n",
    "        config = tf.ConfigProto(log_device_placement=True)\n",
    "\n",
    "        # Start the session to compute the tf graph\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            # Initialization\n",
    "            sess.run(init)\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(num_epochs):\n",
    "                tic = time.time()\n",
    "\n",
    "                minibatch_cost = 0.\n",
    "                num_minibatches = int(m / minibatch_size)  # number of minibatches\n",
    "                seed += 1\n",
    "                minibatches = random_mini_batches(X_train, Y_train,\n",
    "                                                  minibatch_size, seed)\n",
    "                # Minibatch loop\n",
    "                for minibatch in minibatches:\n",
    "                    # Select a minibatch\n",
    "                    (minibatch_X, minibatch_Y) = minibatch\n",
    "                    # Run the session to execute the optimizer and the cost\n",
    "                    _, temp_cost = sess.run(\n",
    "                        [optimizer, cost],\n",
    "                        feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "                    cost_mean = np.mean(temp_cost) / num_minibatches\n",
    "                    minibatch_cost += cost_mean\n",
    "\n",
    "                # Print the cost every epoch\n",
    "                if print_cost:\n",
    "                    toc = time.time()\n",
    "                    print ('EPOCH = ', epoch, 'COST = ', minibatch_cost, 'Elapsed time = ', (toc - tic))\n",
    "\n",
    "            # Save the variables to disk.\n",
    "            save_path = saver.save(sess, \"path to save model/model_name.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "            sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
